data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.val <- data[sampleind == 2,]
#Initialization
cost.test <- runif(9, min = 0, max = 10) #Test cost of each attribute
cost.test <- c(0, cost.test)
cost.FP <- 1000
cost.FN <- 1000
A.known <- c() #Used attributes
A.unknown <- 2:10 #Unused attributes
cost.exp.test <- 0 #Cumulative test cost
funcstr <- "Class ~" #function string
cost.exp <- rep(0, 9) #Record experiment cost with 1-9 attributes
for (i in 1:9){
cost.min <- max(cost.FP, cost.FN) * nrow(data.val) * 2 #A large number
cost.argmin <- 0
for (ind in A.unknown){
fs.temp <- paste(funcstr, "+", attr.names[ind]) #Temp function string
func <- as.formula(fs.temp)
model.temp <- randomForest(func,data=data.train,distribution="bernoulli",n.trees=5000,interaction.depth=4) #Fit model
pred.temp <- predict(model.temp,data.val,n.trees=5000, type="response") #Predict labels
fp <- sum(pred.temp == 2 & data.val$Class == 4) #False positive
fn <- sum(pred.temp == 4 & data.val$Class == 2) #False negative
cost.temp <- cost.test[ind] * nrow(data.val) + fp * cost.FP + fn * cost.FN
if (cost.temp < cost.min){
cost.min <- cost.temp
cost.argmin <- ind
}
}
cost.exp[i] <- cost.exp.test + cost.min # Previous test + current test
cost.exp.test <- cost.exp.test + cost.test[cost.argmin] * nrow(data.val)
funcstr <- paste(funcstr, "+", attr.names[cost.argmin])
A.known <- c(A.known, cost.argmin) #Include new test in the known attribute
A.unknown <- A.unknown[A.unknown != cost.argmin] #Exclude new test in the unknown attribute
}
cost.exp <- cost.exp / nrow(data.val)
cost.exp
num <-1:9
plot(num,cost.exp,type="o",main="Cost of Boosting",xlab="Number of Attributes",ylab="Cost",lwd=2)
axis(1,at=seq(1,10))
library(tree)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.val <- data[sampleind == 2,]
#Initialization
cost.test <- runif(9, min = 0, max = 10) #Test cost of each attribute
cost.test <- c(0, cost.test)
cost.FP <- 1000
cost.FN <- 1000
A.known <- c() #Used attributes
A.unknown <- 2:10 #Unused attributes
cost.exp.test <- 0 #Cumulative test cost
funcstr <- "Class ~" #function string
cost.exp <- rep(0, 9) #Record experiment cost with 1-9 attributes
for (i in 1:9){
cost.min <- max(cost.FP, cost.FN) * nrow(data.val) * 2 #A large number
cost.argmin <- 0
for (ind in A.unknown){
fs.temp <- paste(funcstr, "+", attr.names[ind]) #Temp function string
func <- as.formula(fs.temp)
model.temp <- tree(func, data = data.train) #Fit model
pred.temp <- predict(model.temp, data.val, type = "class") #Predict labels
fp <- sum(pred.temp == 2 & data.val$Class == 4) #False positive
fn <- sum(pred.temp == 4 & data.val$Class == 2) #False negative
cost.temp <- cost.test[ind] * nrow(data.val) + fp * cost.FP + fn * cost.FN
if (cost.temp < cost.min){
cost.min <- cost.temp
cost.argmin <- ind
}
}
cost.exp[i] <- cost.exp.test + cost.min # Previous test + current test
cost.exp.test <- cost.exp.test + cost.test[cost.argmin] * nrow(data.val)
funcstr <- paste(funcstr, "+", attr.names[cost.argmin])
A.known <- c(A.known, cost.argmin) #Include new test in the known attribute
A.unknown <- A.unknown[A.unknown != cost.argmin] #Exclude new test in the unknown attribute
}
cost.exp <- cost.exp / nrow(data.val)
cost.exp
num<-1:9
plot(num,cost.exp,type="o",xlab="Number of Attributes",ylab="Cost",lwd=2,main="Cost of Tree")
axis(1,at=seq(1,10))
library(MASS)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.val <- data[sampleind == 2,]
#Initialization
cost.test <- runif(9, min = 0, max = 10) #Test cost of each attribute
cost.test <- c(0, cost.test)
cost.FP <- 1000
cost.FN <- 1000
A.known <- c() #Used attributes
A.unknown <- 2:10 #Unused attributes
cost.exp.test <- 0 #Cumulative test cost
funcstr <- "Class ~" #function string
cost.exp <- rep(0, 9) #Record experiment cost with 1-9 attributes
num.err <- rep(0, 9)
for (i in 1:9){
cost.min <- max(cost.FP, cost.FN) * nrow(data.val) * 2 #A large number
cost.argmin <- 0
for (ind in A.unknown){
fs.temp <- paste(funcstr, "+", attr.names[ind]) #Temp function string
func <- as.formula(fs.temp)
model.temp <- lda(func, data = data.train) #Fit model
pred.temp <- predict(model.temp, data.val, type = "class") #Predict labels
fp <- sum(pred.temp$class == 2 & data.val$Class == 4) #False positive
fn <- sum(pred.temp$class == 4 & data.val$Class == 2) #False negative
cost.temp <- cost.test[ind] * nrow(data.val) + fp * cost.FP + fn * cost.FN
if (cost.temp < cost.min){
cost.min <- cost.temp
cost.argmin <- ind
}
}
num.err[i] <- fp+fn
cost.exp[i] <- cost.exp.test + cost.min # Previous test + current test
cost.exp.test <- cost.exp.test + cost.test[cost.argmin] * nrow(data.val)
funcstr <- paste(funcstr, "+", attr.names[cost.argmin])
A.known <- c(A.known, cost.argmin) #Include new test in the known attribute
A.unknown <- A.unknown[A.unknown != cost.argmin] #Exclude new test in the unknown attribute
}
cost.exp <- cost.exp / nrow(data.val)
cost.exp
num.err
num<-1:9
plot(num,cost.exp,type="o",xlab="Number of Attributes",ylab="Cost",lwd=2,main="Cost of LDA")
axis(1,at=seq(1,10))
text(cost.exp)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.val <- data[sampleind == 2,]
#Initialization
cost.test <- runif(9, min = 0, max = 10) #Test cost of each attribute
cost.test <- c(0, cost.test)
cost.FP <- 1000
cost.FN <- 1000
A.known <- c() #Used attributes
A.unknown <- 2:10 #Unused attributes
cost.exp.test <- 0 #Cumulative test cost
funcstr <- "Class ~" #function string
cost.exp <- rep(0, 9) #Record experiment cost with 1-9 attributes
for (i in 1:9){
cost.min <- max(cost.FP, cost.FN) * nrow(data.val) * 2 #A large number
cost.argmin <- 0
for (ind in A.unknown){
fs.temp <- paste(funcstr, "+", attr.names[ind]) #Temp function string
func <- as.formula(fs.temp)
model.temp <- glm(func,data=data.train,family=binomial) #Fit model
pred <- predict(model.temp,data.val,type="response") #Predict labels
pred.temp <- ifelse(pred>0.5,4,2)
fp <- sum(pred.temp == 2 & data.val$Class == 4) #False positive
fn <- sum(pred.temp == 4 & data.val$Class == 2) #False negative
cost.temp <- cost.test[ind] * nrow(data.val) + fp * cost.FP + fn * cost.FN
if (cost.temp < cost.min){
cost.min <- cost.temp
cost.argmin <- ind
}
}
cost.exp[i] <- cost.exp.test + cost.min # Previous test + current test
cost.exp.test <- cost.exp.test + cost.test[cost.argmin] * nrow(data.val)
funcstr <- paste(funcstr, "+", attr.names[cost.argmin])
A.known <- c(A.known, cost.argmin) #Include new test in the known attribute
A.unknown <- A.unknown[A.unknown != cost.argmin] #Exclude new test in the unknown attribute
}
cost.exp <- cost.exp / nrow(data.val)
cost.exp
num <-1:9
plot(num,cost.exp,type="o",main="Cost of Logistic Regression",xlab="Number of Attributes",ylab="Cost",lwd=2)
axis(1,at=seq(1,10))
library(MASS)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.val <- data[sampleind == 2,]
#Initialization
cost.test <- runif(9, min = 0, max = 10) #Test cost of each attribute
cost.test <- c(0, cost.test)
cost.FP <- 1000
cost.FN <- 1000
A.known <- c() #Used attributes
A.unknown <- 2:10 #Unused attributes
cost.exp.test <- 0 #Cumulative test cost
funcstr <- "Class ~" #function string
cost.exp <- rep(0, 9) #Record experiment cost with 1-9 attributes
for (i in 1:9){
cost.min <- max(cost.FP, cost.FN) * nrow(data.val) * 2 #A large number
cost.argmin <- 0
for (ind in A.unknown){
fs.temp <- paste(funcstr, "+", attr.names[ind]) #Temp function string
func <- as.formula(fs.temp)
model.temp <- qda(func, data = data.train) #Fit model
pred.temp <- predict(model.temp, data.val, type = "class") #Predict labels
fp <- sum(pred.temp$class == 2 & data.val$Class == 4) #False positive
fn <- sum(pred.temp$class == 4 & data.val$Class == 2) #False negative
cost.temp <- cost.test[ind] * nrow(data.val) + fp * cost.FP + fn * cost.FN
if (cost.temp < cost.min){
cost.min <- cost.temp
cost.argmin <- ind
}
}
cost.exp[i] <- cost.exp.test + cost.min # Previous test + current test
cost.exp.test <- cost.exp.test + cost.test[cost.argmin] * nrow(data.val)
funcstr <- paste(funcstr, "+", attr.names[cost.argmin])
A.known <- c(A.known, cost.argmin) #Include new test in the known attribute
A.unknown <- A.unknown[A.unknown != cost.argmin] #Exclude new test in the unknown attribute
}
cost.exp <- cost.exp / nrow(data.val)
cost.exp
num<-1:9
plot(num,cost.exp,type="o",xlab="Number of Attributes",ylab="Cost",lwd=2,main="Cost of QDA")
axis(1,at=seq(1,10))
library(randomForest)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
#attach(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.val <- data[sampleind == 2,]
#Initialization
cost.test <- runif(9, min = 0, max = 10) #Test cost of each attribute
cost.test <- c(0, cost.test)
cost.FP <- 1000
cost.FN <- 1000
A.known <- c() #Used attributes
A.unknown <- 2:10 #Unused attributes
cost.exp.test <- 0 #Cumulative test cost
funcstr <- "Class ~" #function string
cost.exp <- rep(0, 9) #Record experiment cost with 1-9 attributes
#model.temp <- randomForest(Class~CT, data = data.train, mtry=3)
for (i in 1:9){
cost.min <- max(cost.FP, cost.FN) * nrow(data.val) * 2 #A large number
cost.argmin <- 0
for (ind in A.unknown){
fs.temp <- paste(funcstr, "+", attr.names[ind]) #Temp function string
func <- as.formula(fs.temp)
model.temp <- randomForest(func, data = data.train, mtry=3) #Fit model
pred.temp <- predict(model.temp, newdata = data.val, type = "class") #Predict labels
fp <- sum(pred.temp == 2 & data.val$Class == 4) #False positive
fn <- sum(pred.temp == 4 & data.val$Class == 2) #False negative
cost.temp <- cost.test[ind] * nrow(data.val) + fp * cost.FP + fn * cost.FN
if (cost.temp < cost.min){
cost.min <- cost.temp
cost.argmin <- ind
}
}
cost.exp[i] <- cost.exp.test + cost.min # Previous test + current test
cost.exp.test <- cost.exp.test + cost.test[cost.argmin] * nrow(data.val)
funcstr <- paste(funcstr, "+", attr.names[cost.argmin])
A.known <- c(A.known, cost.argmin) #Include new test in the known attribute
A.unknown <- A.unknown[A.unknown != cost.argmin] #Exclude new test in the unknown attribute
}
cost.exp <- cost.exp / nrow(data.val)
cost.exp
num <-1:9
plot(num,cost.exp,type="o",xlab="Number of Attributes",ylab="Cost",lwd=2,main="Cost of RandomForest")
axis(1,at=seq(1,10))
library(e1071)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.val <- data[sampleind == 2,]
#Initialization
cost.test <- runif(9, min = 0, max = 10) #Test cost of each attribute
cost.test <- c(0, cost.test)
cost.FP <- 1000
cost.FN <- 1000
A.known <- c() #Used attributes
A.unknown <- 2:10 #Unused attributes
cost.exp.test <- 0 #Cumulative test cost
funcstr <- "Class ~" #function string
cost.exp <- rep(0, 9) #Record experiment cost with 1-9 attributes
for (i in 1:9){
cost.min <- max(cost.FP, cost.FN) * nrow(data.val) * 2 #A large number
cost.argmin <- 0
for (ind in A.unknown){
fs.temp <- paste(funcstr, "+", attr.names[ind]) #Temp function string
func <- as.formula(fs.temp)
model.temp <- svm(func, data = data.train, kernel = "linear") #Fit model
pred.temp <- predict(model.temp, data.val) #Predict labels
fp <- sum(pred.temp == 2 & data.val$Class == 4) #False positive
fn <- sum(pred.temp == 4 & data.val$Class == 2) #False negative
cost.temp <- cost.test[ind] * nrow(data.val) + fp * cost.FP + fn * cost.FN
if (cost.temp < cost.min){
cost.min <- cost.temp
cost.argmin <- ind
}
}
cost.exp[i] <- cost.exp.test + cost.min # Previous test + current test
cost.exp.test <- cost.exp.test + cost.test[cost.argmin] * nrow(data.val)
funcstr <- paste(funcstr, "+", attr.names[cost.argmin])
A.known <- c(A.known, cost.argmin) #Include new test in the known attribute
A.unknown <- A.unknown[A.unknown != cost.argmin] #Exclude new test in the unknown attribute
}
cost.exp <- cost.exp / nrow(data.val)
cost.exp
num<-1:9
plot(num,cost.exp,type="o",xlab="Number of Attributes",ylab="Cost",lwd=2,main="Cost of Linear SVM")
axis(1,at=seq(1,10))
library(e1071)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.val <- data[sampleind == 2,]
#Initialization
cost.test <- runif(9, min = 0, max = 10) #Test cost of each attribute
cost.test <- c(0, cost.test)
cost.FP <- 1000
cost.FN <- 1000
A.known <- c() #Used attributes
A.unknown <- 2:10 #Unused attributes
cost.exp.test <- 0 #Cumulative test cost
funcstr <- "Class ~" #function string
cost.exp <- rep(0, 9) #Record experiment cost with 1-9 attributes
for (i in 1:9){
cost.min <- max(cost.FP, cost.FN) * nrow(data.val) * 2 #A large number
cost.argmin <- 0
for (ind in A.unknown){
fs.temp <- paste(funcstr, "+", attr.names[ind]) #Temp function string
func <- as.formula(fs.temp)
model.temp <- svm(func, data = data.train, kernel = "radial") #Fit model
pred.temp <- predict(model.temp, data.val) #Predict labels
fp <- sum(pred.temp == 2 & data.val$Class == 4) #False positive
fn <- sum(pred.temp == 4 & data.val$Class == 2) #False negative
cost.temp <- cost.test[ind] * nrow(data.val) + fp * cost.FP + fn * cost.FN
if (cost.temp < cost.min){
cost.min <- cost.temp
cost.argmin <- ind
}
}
cost.exp[i] <- cost.exp.test + cost.min # Previous test + current test
cost.exp.test <- cost.exp.test + cost.test[cost.argmin] * nrow(data.val)
funcstr <- paste(funcstr, "+", attr.names[cost.argmin])
A.known <- c(A.known, cost.argmin) #Include new test in the known attribute
A.unknown <- A.unknown[A.unknown != cost.argmin] #Exclude new test in the unknown attribute
}
cost.exp <- cost.exp / nrow(data.val)
cost.exp
num<-1:9
plot(num,cost.exp,type="o",xlab="Number of Attributes",ylab="Cost",lwd=2,main="Cost of Radial SVM")
axis(1,at=seq(1,10))
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.test <- data[sampleind == 2,]
#Decision Tree
library(tree)
model.tree <- tree(Class ~ .-Index, data = data.train)
#summary(model.tree)
plot(model.tree)
text(model.tree)
pred.tree <- predict(model.tree, newdata=data.test, type = "class")
table(pred.tree, data.test$Class)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.test <- data[sampleind == 2,]
#Decision Tree
library(tree)
model.tree <- tree(Class ~ .-Index, data = data.train)
#summary(model.tree)
plot(model.tree)
text(model.tree)
pred.tree <- predict(model.tree, newdata=data.test, type = "class")
table(pred.tree, data.test$Class)
#linear/radial SVM
library(e1071)
svmfit.linear <- svm(Class ~.-Index, data = data.train, kernel = "linear")
svmfit.radial <- svm(Class ~.-Index, data = data.train, kernel = "radial")
pred.svm.linear <- predict(svmfit.linear, newdata=data.test)
table(pred.svm.linear, data.test$Class)
pred.svm.radial <- predict(svmfit.radial, newdata=data.test)
table(pred.svm.radial, data.test$Class)
set.seed(6690)
attr.names <- c("Index","CT","CellSize","CellShape","MA","SE_CellSize","BN","BC","NN","Mitoses","Class")
data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", header = F,
col.names = attr.names, na.strings = "?")
data <- na.omit(data)
data$Class <- as.factor(data$Class)
#Partition train 70% test 30%
sampleind <- sample(2, nrow(data), replace = T, prob = c(0.7, 0.3))
data.train <- data[sampleind == 1,]
data.test <- data[sampleind == 2,]
#Decision Tree
library(tree)
model.tree <- tree(Class ~ .-Index, data = data.train)
#summary(model.tree)
plot(model.tree)
text(model.tree)
pred.tree <- predict(model.tree, newdata=data.test, type = "class")
table(pred.tree, data.test$Class)
#linear/radial SVM
library(e1071)
svmfit.linear <- svm(Class ~.-Index, data = data.train, kernel = "linear")
svmfit.radial <- svm(Class ~.-Index, data = data.train, kernel = "radial")
pred.svm.linear <- predict(svmfit.linear, newdata=data.test)
table(pred.svm.linear, data.test$Class)
pred.svm.radial <- predict(svmfit.radial, newdata=data.test)
table(pred.svm.radial, data.test$Class)
#Boosting
library(gbm)
Classi=ifelse(Class==2,0,1)
data=data.frame(data,Classi)
data.train.boost = data[sampleind == 1,]
data.test.boost = data[sampleind == 2,]
boost.fit=gbm(Classi~.-Index-Class,data=data.train.boost,distribution="bernoulli",n.trees=5000,interaction.depth=4)
boost.pred=predict(boost.fit,newdata=data.test.boost,n.trees=5000, type="response")
boost.predict_class=ifelse(boost.pred > 0.5,1,0)
table(boost.predict_class,data.test.boost$Class)
#Logistic Regression
attach(data)
#Classi=ifelse(Class==2,0,1)
#data=data.frame(data,Classi)
glm.fits=glm(Class~.-Index,data=data.train,family=binomial)
glm.pred=predict(glm.fits,newdata=data.test,type="response")
glm.predict_class <- ifelse(glm.pred>0.5,1,0)
table(glm.predict_class,data.test$Class)
#Bagging
library(randomForest)
#attach(data)
bag.fit=randomForest(Class~.-Index,data=data.train,mtry=10)
bag.pred = predict(bag.fit,newdata = data.test)
table(bag.pred,data.test$Class)
#LDA
library(MASS)
lda.fit  = lda(Class~.-Index, data = data.train)
#lda.fit
lda.pred = predict(lda.fit, data.test)
lda.class = lda.pred$class
table(lda.class,data.test$Class)
#QDA
qda.fit = qda(Class~.-Index, data = data.train)
qda.fit
qda.pred = predict(qda.fit, data.test)
qda.class = qda.pred$class
table(qda.class,data.test$Class)
#Random Forest
library(randomForest)
#attach(data)
bag.fit=randomForest(Class~.-Index,data=data.train,mtry=3)
bag.pred = predict(bag.fit,newdata = data.test)
table(bag.pred,data.test$Class)
#AdaBoost
#install.packages("ggplot2",dependencies=TRUE)
library(adabag)
library(ggplot2)
#attach(data)
data$Class <- as.factor(data$Class)
data.train$Class <- as.factor(data.train$Class)
data.test$Class <- as.factor(data.test$Class)
adaboost<-boosting(Class~.-Index, data=data.train, boos=TRUE, mfinal=20,coeflearn='Breiman')
#for(i in 1:20){
#  data.fit <- boosting(Class~., data=data, mfinal=i)
#  data.pred <- predict.boosting(data.fit,data = data)
#}
ada.pred = predict(adaboost,data.test)
ada.pred$confusion
